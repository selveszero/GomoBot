{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial11basedDQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ddajjvySiA"
      },
      "source": [
        "# imports\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn \n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asUY88Z-yR3W"
      },
      "source": [
        "**Gomoku** Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39cDAMATEfRU"
      },
      "source": [
        "class Gomoku:\n",
        "  def __init__(self, dim, win):\n",
        "    self.dim = dim\n",
        "    self.win = dim\n",
        "    self.total_moves = 0\n",
        "    self.winner = 0\n",
        "    self.board = np.zeros((self.dim, self.dim))\n",
        "\n",
        "  def get_available(self):\n",
        "    positions = list(map(list, np.where(self.board == 0)))\n",
        "    return list(zip(positions[0], positions[1]))\n",
        "\n",
        "  def make_move(self, position, player):\n",
        "    if position not in self.get_available():\n",
        "      return -1000\n",
        "      \n",
        "    self.board[position] = player\n",
        "    self.total_moves += 1 \n",
        "\n",
        "    if self.won() == self.win:\n",
        "      self.winner = player\n",
        "      return 1000\n",
        "    \n",
        "    return -10\n",
        "\n",
        "  def won(self):\n",
        "    # Return self.winner if there's a winning subsequence for any player. \n",
        "    # check horizontal\n",
        "    ans = 0\n",
        "\n",
        "    for i in range(0, self.dim):\n",
        "    #  print([ abs(sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)])\n",
        "      ans = max(ans, max([ abs(sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    for j in range(0, self.dim):\n",
        "      ans  =max(ans, max([ abs(sum(self.board[i: i+self.win, j])) for i in range(0, self.dim - self.win + 1)]))\n",
        "    #check vertical\n",
        "\n",
        "    for i in range(0, self.dim - self.win+1):\n",
        "      ans = max(ans, max([ abs(sum(   self.board[ [i + k for k in range(0, self.win)], [j + l for l in range(0, self.win)]  ]    )) for j in range(0, self.dim - self.win + 1 )]))\n",
        "    #check diagonal\n",
        "    for i in range(self.win-1, self.dim):\n",
        "      ans = max(ans, max([ abs(sum(self.board[[i - k for k in range(0, self.win)], [j + l for l in range(0, self.win)] ])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    #check anti-diagonal\n",
        "    return ans\n",
        "\n",
        " "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF7wtOmzswXU"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDxMLwpveE83"
      },
      "source": [
        "#Network for Q learning\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_channels, n_actions):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels=n_channels, out_channels=16,\n",
        "                          kernel_size=5, stride=1)\n",
        "    self.fc1 = nn.Linear(in_features=1024, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=n_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv(x))\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLkyqFwd4jS"
      },
      "source": [
        "#Agent class as in week 11 tutorial.\n",
        "class Agents:\n",
        "  def __init__(self, policy1, policy2, p1_net, p2_net, optimizer1, optimizer2):\n",
        "    self.policy1 = policy1\n",
        "    self.policy2 = policy2\n",
        "    self.q1 = p1_net\n",
        "    self.q2 = p2_net\n",
        "    self.optimizer1 = optimizer1\n",
        "    self.optimizer2 = optimizer2\n",
        "\n",
        "  def act(self, state, player):\n",
        "    with torch.no_grad():\n",
        "      if player == 1 :\n",
        "        return self.policy1(self.q1, state)\n",
        "      else:\n",
        "        return self.policy2(self.q2, state)\n",
        "  \n",
        "  def train(self, state, action, player, reward, discount, next_state):\n",
        "    if player == 1:\n",
        "      q_net = self.q1\n",
        "      optimizer = self.optimizer1\n",
        "    else:\n",
        "      q_net = self.q2\n",
        "      optimizer = self.optimizer2\n",
        "\n",
        "    q_pred = q_net(state).gather(1, action)\n",
        "    with torch.no_grad():\n",
        "      q_target = q_net(next_state).max(dim=1)[0].view(-1, 1)\n",
        "      q_target = reward + discount * q_target\n",
        "    \n",
        "    loss = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYOvgkXteD_N"
      },
      "source": [
        "def train_agents(agents, gamma, num_games, dim, num_win):\n",
        "  \n",
        "  for _ in range(num_games):\n",
        "    game = Gomoku(dim, num_win)\n",
        "    curr_player = 1\n",
        "    end_game = 0 \n",
        "    rewards = {1: 0, -1:0}\n",
        "    while game.winner == 0 and game.total_moves < dim**2:\n",
        "      #let two agents play each other and learn.\n",
        "      state = game.board\n",
        "      action = agents.act(state, curr_player)\n",
        "      reward = game.make_move(action, curr_player)\n",
        "      end_game, next_state = abs(game.winner), game.board\n",
        "      discount = gamma*(1-end_game)\n",
        "\n",
        "      agents.train(state, action, curr_player, reward, discount, next_state)\n",
        "\n",
        "      rewards[curr_player] += reward\n",
        "\n",
        "      state = next_State\n",
        "      \n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLOVcCmndydo"
      },
      "source": [
        "def epsilon_greedy(n_actions, epsilon):\n",
        "  def policy_fn(q_net, state):\n",
        "    if torch.rand(1) < epsilon:\n",
        "      return torch.randint(n_actions, size=(1,), device=device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        q_pred = q_net(state)\n",
        "        return torch.argmax(q_pred).view(1,)\n",
        "  return policy_fn"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOARqBngBPXS"
      },
      "source": [
        "num_games = 100\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "dim = 3\n",
        "num_win = 3\n",
        "num_actions = \n",
        "\n",
        "p1 = DQN(dim, num_win)\n",
        "p2 = DQN(dim, num_win)\n",
        "\n",
        "\n",
        "policy1 = epsilon_greedy(num_actions, epsilon)\n",
        "policy2 = epsilon_greedy(num_actions, epsilon)\n",
        "\n",
        "optimizer1 = torch.optim.Adam(p1.parameters(), lr=1e-3)\n",
        "optimizer2 = torch.optim.Adam(p2.parameters(), lr=1e-3)\n",
        "\n",
        "agents = Agents(policy1, policy2, p1, p2, optimizer1, optimizer2)\n",
        "train_agents(agents, gamma, num_games, dim, num_win)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(eps_b_qn[0])\n",
        "plt.title('breakout reward curve')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('return')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}