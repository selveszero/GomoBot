{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Gomoku_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSIJjEngw-Ip"
      },
      "source": [
        "# imports\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn \n",
        "import numpy as np\n",
        "from typing import NamedTuple\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RcJVOgYw-yS"
      },
      "source": [
        "class Gomoku:\n",
        "  def __init__(self, dim, win):\n",
        "    self.dim = dim\n",
        "    self.win = win\n",
        "    self.total_moves = 0\n",
        "    self.winner = 0\n",
        "    self.end = 0\n",
        "    self.board = np.zeros((self.dim, self.dim))\n",
        "    self.available = np.zeros((self.dim, self.dim))\n",
        "\n",
        "    self.log = [] \n",
        "\n",
        "  def get_available_coord(self):\n",
        "    positions = list(map(list, np.where(self.board == 0)))\n",
        "    return list(zip(positions[0], positions[1]))\n",
        "\n",
        "  def get_available_action(self):\n",
        "    temp = self.get_available_coord()\n",
        "    return [self.coordinates_to_action(x) for x in temp]\n",
        "\n",
        "\n",
        "  def action_to_coordinates(self, action):\n",
        "    return ((action//self.dim), (action%self.dim))\n",
        "\n",
        "  def coordinates_to_action(self, coord):\n",
        "    return self.dim * coord[0] + coord[1]\n",
        "\n",
        "\n",
        "  def state_after_move(self, position, player):\n",
        "    new_state = copy.deepcopy(self.board)\n",
        "    new_available = copy.deepcopy(self.available)\n",
        "    tot = self.total_moves\n",
        "    reward = -1\n",
        "    terminate = 0 \n",
        "\n",
        "    if position not in self.get_available_coord():\n",
        "      terminate = 1\n",
        "      reward = -10\n",
        "      \n",
        "    new_state[position] = player\n",
        "    new_available[position] = float(\"-inf\")\n",
        "\n",
        "\n",
        "    if tot + 1 == dim**2:\n",
        "      terminate = 1\n",
        "\n",
        "    if self.won2(new_state) == self.win:\n",
        "      terminate = 1\n",
        "      reward = 10\n",
        "    \n",
        "    return new_state, new_available, tot, terminate, reward\n",
        "\n",
        "  def make_move(self, position, player):\n",
        "    self.log.append((player, position))\n",
        "    if position not in self.get_available_coord():\n",
        "      self.end = 1\n",
        "      return -10\n",
        "      \n",
        "    self.board[position] = player\n",
        "    self.available[position] = float(\"-inf\")\n",
        "    self.total_moves += 1 \n",
        "    # self.available\n",
        "\n",
        "    consec = self.won()\n",
        "    chains = {1 : consec[0], -1 : consec[1]}\n",
        "    \n",
        "    if abs(chains[player*(-1)]) == self.win - 1:\n",
        "      return -10\n",
        "\n",
        "    if abs(chains[player]) ==  self.win:\n",
        "      self.winner = player\n",
        "      self.end = 1\n",
        "      return 10\n",
        "\n",
        "    if self.total_moves == dim**2:\n",
        "      self.end = 1\n",
        "      return 0\n",
        "\n",
        "    \n",
        "    return -1\n",
        "\n",
        "  def won(self):\n",
        "    # Return self.winner if there's a winning subsequence for any player. \n",
        "    # check horizontal\n",
        "    ans = 0\n",
        "    ans2 = 0 \n",
        "    for i in range(0, self.dim):\n",
        "      ans = max(ans, max([ (sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)]))\n",
        "      ans2 = min(ans2, min([ (sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    #check vertical\n",
        "    for j in range(0, self.dim):\n",
        "      ans  =max(ans, max([ (sum(self.board[i: i+self.win, j])) for i in range(0, self.dim - self.win + 1)]))\n",
        "      ans2  =min(ans2, min([ (sum(self.board[i: i+self.win, j])) for i in range(0, self.dim - self.win + 1)]))\n",
        "  \n",
        "    for i in range(0, self.dim - self.win+1):\n",
        "      ans = max(ans, max([ (sum(   self.board[ [i + k for k in range(0, self.win)], [j + l for l in range(0, self.win)]  ]    )) for j in range(0, self.dim - self.win + 1 )]))\n",
        "      ans2 = min(ans2, min([ (sum(   self.board[ [i + k for k in range(0, self.win)], [j + l for l in range(0, self.win)]  ]    )) for j in range(0, self.dim - self.win + 1 )]))\n",
        "    #check diagonal\n",
        "    for i in range(self.win-1, self.dim):\n",
        "      ans = max(ans, max([ (sum(self.board[[i - k for k in range(0, self.win)], [j + l for l in range(0, self.win)] ])) for j in range(0, self.dim - self.win + 1)]))\n",
        "      ans2 = min(ans2, min([ (sum(self.board[[i - k for k in range(0, self.win)], [j + l for l in range(0, self.win)] ])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    #check anti-diagonal\n",
        "    return ans, ans2\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BECmSA-xOfj"
      },
      "source": [
        "#Network for Q learning\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, dim, num_win):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(dim*dim, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "    self.fc5 = nn.Linear(32, dim*dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc5(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLnPa5s0xpRU"
      },
      "source": [
        "def epsilon_greedy(epsilon):\n",
        "  def policy_fn(available_actions, available_board, q_net, state):\n",
        "    if torch.rand(1) < epsilon:\n",
        "      return torch.tensor([np.random.choice(available_actions)] , device = device)\n",
        "    else:\n",
        "      with torch.no_grad():  \n",
        "        q_pred = q_net(torch.flatten(state))\n",
        "        return torch.argmax(q_pred + available_board.view(1,-1) ).view(1,)\n",
        "  return policy_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNXvVpCdyjWg"
      },
      "source": [
        "def random_policy():\n",
        "  def policy_fn(available_actions,available_board = None, q_net = None, state= None):\n",
        "    return torch.tensor([np.random.choice(available_actions)] , device = device)\n",
        "  return policy_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cj45HN6zlVu"
      },
      "source": [
        "class Batch(NamedTuple):\n",
        "  state: torch.Tensor\n",
        "  action: torch.Tensor\n",
        "  reward: torch.Tensor\n",
        "  discount: torch.Tensor\n",
        "  next_state: torch.Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrQIwZUIztu4"
      },
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, state_dim, act_dim, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.ptr = 0\n",
        "    self.n_samples = 0\n",
        "\n",
        "    self.state = torch.zeros(buffer_size, *state_dim, dtype=torch.float32, device=device)\n",
        "    self.action = torch.zeros(buffer_size, act_dim, dtype=torch.int64, device=device)\n",
        "    self.reward = torch.zeros(buffer_size, 1, dtype=torch.float32, device=device)\n",
        "    self.discount = torch.zeros(buffer_size, 1, dtype=torch.float32, device=device)\n",
        "    self.next_state = torch.zeros(buffer_size, *state_dim, dtype=torch.float32, device=device)\n",
        "\n",
        "  def add(self, state, action, reward, discount, next_state):\n",
        "    self.state[self.ptr] = state\n",
        "    self.action[self.ptr] = action\n",
        "    self.reward[self.ptr] = reward\n",
        "    self.discount[self.ptr] = discount\n",
        "    self.next_state[self.ptr] = next_state\n",
        "    \n",
        "    if self.n_samples < self.buffer_size:\n",
        "      self.n_samples += 1\n",
        "\n",
        "    self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    # Select batch_size number of sample indicies at random from the buffer\n",
        "    idx = np.random.choice(self.n_samples, batch_size)\n",
        "    state = self.state[idx]\n",
        "    action = self.action[idx]\n",
        "    reward = self.reward[idx]\n",
        "    discount = self.discount[idx]\n",
        "    next_state = self.next_state[idx]\n",
        "\n",
        "    return Batch(state, action, reward, discount, next_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUnLoecx3s9g"
      },
      "source": [
        "#Agent class as in week 11 tutorial.\n",
        "class Agent:\n",
        "  def __init__(self, policy, qnet= None, optimizer = None, replay_buffer= None, batch_size = None):\n",
        "    self.policy = policy\n",
        "    self.qnet = qnet\n",
        "    self.optimizer = optimizer  \n",
        "    self.replay_buffer = replay_buffer\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def act(self, available_actions, available_board, state):\n",
        "    with torch.no_grad():\n",
        "      return self.policy(available_actions, available_board, self.qnet, state)\n",
        "  \n",
        "  def train(self, state, action, reward, discount, next_state, i):\n",
        "\n",
        "    self.replay_buffer.add(state, action, reward, discount, next_state)  \n",
        "\n",
        "\n",
        "    batch = self.replay_buffer.sample(self.batch_size)\n",
        "    q_actions = self.qnet(torch.flatten(batch.state, start_dim = 1))\n",
        "    q_pred = q_actions.gather(1, batch.action)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "      print(q_pred)\n",
        "    with torch.no_grad():\n",
        "      q_target_actions = self.qnet(torch.flatten(batch.next_state, start_dim = 1))\n",
        "      q_target = q_target_actions.max(dim=1)[0].view(-1, 1)\n",
        "      q_target = batch.discount * q_target + batch.reward\n",
        "      \n",
        "\n",
        "    #print(q_pred.shape, q_target.shape)\n",
        "    loss = F.mse_loss(q_target, q_pred).mean()\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8mlOXl8L662"
      },
      "source": [
        "\n",
        "# #Agent class as in week 11 tutorial.\n",
        "# class Agent:\n",
        "#   def __init__(self, policy, qnet, optimizer):\n",
        "#     self.policy = policy\n",
        "#     self.qnet = qnet\n",
        "#     self.optimizer = optimizer  \n",
        "\n",
        "#   def act(self, available_random, available, state):\n",
        "#     with torch.no_grad():\n",
        "#       return self.policy(available_random, available, self.qnet, state)\n",
        "  \n",
        "#   def train(self, state, action, reward, gamma, game, player, rival):\n",
        "    \n",
        "#     if reward == 1 or reward == 0:\n",
        "#       q_pred = self.qnet(state).view(1,-1)\n",
        "#       actionr = action.view(1,-1)\n",
        "#       q_pred = q_pred.gather(1, actionr)\n",
        "#       with torch.no_grad():\n",
        "#         q_target = torch.tensor([[reward]]).type(torch.float32).to(device)\n",
        "#         if q_pred > 10000000:\n",
        "#           print(q_pred, q_target)\n",
        "#     else:\n",
        "#       q_pred = self.qnet(state).view(1,-1)\n",
        "#       actionr = action.view(1,-1)\n",
        "#       q_pred = q_pred.gather(1, actionr)\n",
        "\n",
        "#       position = ((action//dim).cpu().data.numpy()[0], (action%dim).cpu().data.numpy()[0])\n",
        "#       new_state, new_available, tot, terminate, rewardd =  game.state_after_move(position, player)\n",
        "#       new_state = torch.from_numpy(new_state).type(torch.float32).to(device)\n",
        "#       q_target = self.qnet(new_state).view(1,-1)\n",
        "#       q_target = torch.max(q_target).view(-1,1)\n",
        "#       q_target = reward + gamma * q_target\n",
        "\n",
        "      \n",
        "#     #   q_pred = self.qnet(state).view(1,-1)\n",
        "\n",
        "#     #  # print(f\"q-values for player {player} is:\")\n",
        "#     # #  print(q_pred.view(3,3))\n",
        "#     #   actionr = action.view(1,-1)\n",
        "#     #   q_pred = q_pred.gather(1, actionr)\n",
        "\n",
        "#     #   with torch.no_grad():\n",
        "#     #     board_for_rival = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "#     #     #print(\"For rival\", game.board, game.get_available_action())\n",
        "#     #     available_for_rival_random = np.array(game.get_available_action())\n",
        "#     #     available_for_rival = game.available\n",
        "\n",
        "\n",
        "#     #     rival_q = rival.qnet(board_for_rival).view(1,-1)\n",
        "#     #     rival_action = rival.act(available_for_rival_random, available_for_rival, board_for_rival)\n",
        "#     #     rival_position =  ((rival_action//dim).cpu().data.numpy()[0], (rival_action%dim).cpu().data.numpy()[0])\n",
        "#     #     new_state, new_available, tot, terminate, riv_reward =  game.state_after_move(rival_position, player*-1)\n",
        "#     #     if riv_reward == 10:\n",
        "#     #       q_target = -1* torch.tensor([[-1 * reward]]).type(torch.float32).to(device)\n",
        "#     #     elif terminate == 1:\n",
        "#     #       q_target = reward + gamma * q_pred\n",
        "#     #     else:\n",
        "#     #       q_riv = torch.from_numpy(available_for_rival).type(torch.float32).view(1,-1).to(device) - rival_q\n",
        "#     #       #print(q_riv)\n",
        "#     #       q_riv = torch.max(q_riv).view(-1,1)\n",
        "\n",
        "#     #       new_state = torch.from_numpy(new_state).type(torch.float32).to(device)  \n",
        "#     #    #   print(f\"player {player} playing\")\n",
        "#     #     #  print(state.cpu().data.numpy())\n",
        "#     #     #  print(new_state.cpu().data.numpy())\n",
        "#     #       q_target = self.qnet(new_state).view(1,-1)\n",
        "#     #   #   print(f\"target q-values for player {player} is:\")\n",
        "#     #     #  print(q_target.view(3,3))\n",
        "#     #       q_target = torch.max(q_target).view(-1,1)\n",
        "#     #      # print(\"hi\", q_pred, q_riv, q_target)\n",
        "#     #       q_target = reward + gamma * q_target + gamma * q_riv\n",
        "          \n",
        "\n",
        "        \n",
        "#       #print(q_target)\n",
        "#     #print(q_pred, q_target)\n",
        "#   #  print('\\n')\n",
        "#     #print(q_pred.size, q_target.size)\n",
        "#     loss = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "#     self.optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     self.optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDH_dENe1Aj0"
      },
      "source": [
        "def train_agent2(agent1, agent2, gamma, num_games, dim, num_win):\n",
        "  for i in range(num_games):\n",
        "    game = Gomoku(dim, num_win)\n",
        "    curr_player = 1\n",
        "    rewards = {1: 0, -1: 0}\n",
        "    agent = {1 : agent1, -1 : agent2}\n",
        "\n",
        "    while game.end == 0:\n",
        "      #let two agents play each other and learn.\n",
        "      state = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "      available_actions = np.array(game.get_available_action())\n",
        "      available_board = torch.from_numpy(game.available).type(torch.float32).to(device)\n",
        "      #print(curr_player, state)\n",
        "      action = agent[curr_player].act(available_actions, available_board, state)\n",
        "      action_coord = ((action//dim).cpu().data.numpy()[0], (action%dim).cpu().data.numpy()[0])\n",
        "      reward = game.make_move(action_coord, curr_player)\n",
        "      rewards[curr_player] += reward\n",
        "      discount = gamma*(1 - game.end)\n",
        "\n",
        "      next_state = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "\n",
        "      if curr_player == -1:\n",
        "        agent2.train(state, action, reward, discount, next_state, i)\n",
        "      curr_player = curr_player * -1\n",
        "    \n",
        "      \n",
        "    if i % 1000 == 0:\n",
        "      print(f\"game {i} completed\")\n",
        "      print(game.board)\n",
        "      print(game.log)\n",
        "      print(len(game.log))\n",
        "      print(rewards)\n",
        "   # print(game.board, game.winner, game.total_moves)\n",
        "\n",
        "\n",
        "def play_against_agent(agent, dim, num_win):\n",
        "  game = Gomoku(dim, num_win)\n",
        "  curr_player = 1\n",
        "  while game.end == 0:\n",
        "    state = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "    #print(state, curr_player)\n",
        "    if curr_player == -1:\n",
        "      state = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "      available_actions = np.array(game.get_available_action())\n",
        "      available_board = torch.from_numpy(game.available).type(torch.float32).to(device)\n",
        "\n",
        "      action = agent.act(available_actions, available_board, state)\n",
        "      position = ((action//dim).cpu().data.numpy()[0], (action%dim).cpu().data.numpy()[0])\n",
        "      print(f\"Agent has made a move: {position}\")\n",
        "\n",
        "    else:\n",
        "      position = eval(input(\"Enter position:\"))\n",
        "     # print(position.whattype())\n",
        "      print(f\"You have made a move: {position}\")\n",
        "    reward = game.make_move(position, curr_player)\n",
        "    #print(game.end, game.winner)\n",
        "    #end_game, next_state = game.end, torch.from_numpy(game.board).type(torch.float32)\n",
        "   # print(game.end, game.winner)\n",
        "     \n",
        "    print(game.board)\n",
        "    print(\"\\n\")\n",
        "    #rewards[curr_player] += reward\n",
        "    curr_player = curr_player * -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5HzFoffEYqqA",
        "outputId": "d916087c-59ff-43f8-a573-b3e150007762"
      },
      "source": [
        "\n",
        "num_games = 10000\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "dim = 3\n",
        "num_win = 3\n",
        "num_actions = dim**2\n",
        "\n",
        "policy1 = random_policy()\n",
        "policy2 = epsilon_greedy(epsilon)\n",
        "\n",
        "qnet = DQN(dim, num_win).to(device)\n",
        "optimizer = torch.optim.Adam(qnet.parameters(), lr=1e-3)\n",
        "buffer_size = 30\n",
        "batch_size = 9\n",
        "replay_buffer = ReplayBuffer((dim,dim), 1, buffer_size)\n",
        "\n",
        "\n",
        "agent1 = Agent(policy = policy1, qnet= None, optimizer = None, replay_buffer= None, batch_size = None)\n",
        "agent2 = Agent(policy = policy2, qnet= qnet, optimizer = optimizer, replay_buffer= replay_buffer, batch_size = batch_size)\n",
        "\n",
        "train_agent2(agent1, agent2, gamma, num_games, dim, num_win)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199],\n",
            "        [0.1199]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[0.0595],\n",
            "        [0.1117],\n",
            "        [0.0595],\n",
            "        [0.1117],\n",
            "        [0.1117],\n",
            "        [0.1117],\n",
            "        [0.1117],\n",
            "        [0.0595],\n",
            "        [0.0595]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[ 0.0526],\n",
            "        [-0.0239],\n",
            "        [ 0.0526],\n",
            "        [ 0.0526],\n",
            "        [ 0.0526],\n",
            "        [-0.0239],\n",
            "        [ 0.1074],\n",
            "        [ 0.1074],\n",
            "        [ 0.0526]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[ 0.1044],\n",
            "        [-0.0277],\n",
            "        [ 0.0445],\n",
            "        [-0.0277],\n",
            "        [-0.0044],\n",
            "        [ 0.1044],\n",
            "        [ 0.0445],\n",
            "        [-0.0044],\n",
            "        [ 0.1044]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "game 0 completed\n",
            "[[-1.  1.  1.]\n",
            " [-1.  1.  1.]\n",
            " [ 1. -1. -1.]]\n",
            "[(1, (1, 2)), (-1, (2, 1)), (1, (1, 1)), (-1, (0, 0)), (1, (2, 0)), (-1, (1, 0)), (1, (0, 1)), (-1, (2, 2)), (1, (0, 2))]\n",
            "9\n",
            "{1: 6, -1: -31}\n",
            "tensor([[ 94044.3906],\n",
            "        [ 47892.6523],\n",
            "        [171017.8281],\n",
            "        [  8364.6387],\n",
            "        [ 61987.1562],\n",
            "        [  8364.6387],\n",
            "        [195461.8750],\n",
            "        [ 96096.2422],\n",
            "        [109707.2266]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[ 68019.8203],\n",
            "        [ 96230.3359],\n",
            "        [ 58828.7695],\n",
            "        [148760.5938],\n",
            "        [ 68019.8203],\n",
            "        [194328.9062],\n",
            "        [ 41757.9727],\n",
            "        [214182.1250],\n",
            "        [ 57230.8633]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[214601.3594],\n",
            "        [196275.9375],\n",
            "        [  8390.0781],\n",
            "        [ 41871.5391],\n",
            "        [ 94362.2422],\n",
            "        [ 96363.4453],\n",
            "        [ 35427.8906],\n",
            "        [ 48133.8555],\n",
            "        [198564.3438]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[215009.5625],\n",
            "        [149378.9844],\n",
            "        [176152.5781],\n",
            "        [ 96497.2812],\n",
            "        [176152.5781],\n",
            "        [171996.4844],\n",
            "        [ 94519.8984],\n",
            "        [171911.1562],\n",
            "        [ 52879.1641]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "game 1000 completed\n",
            "[[-1.  1. -1.]\n",
            " [ 1. -1. -1.]\n",
            " [ 1.  1.  1.]]\n",
            "[(1, (2, 0)), (-1, (0, 0)), (1, (2, 2)), (-1, (0, 2)), (1, (2, 1)), (-1, (1, 2)), (1, (1, 0)), (-1, (1, 1)), (1, (0, 1))]\n",
            "9\n",
            "{1: -12, -1: -13}\n",
            "tensor([[1906110.5000],\n",
            "        [1029580.2500],\n",
            "        [1215914.7500],\n",
            "        [1358018.2500],\n",
            "        [1088072.2500],\n",
            "        [ 854995.0000],\n",
            "        [ 278209.1562],\n",
            "        [1726932.0000],\n",
            "        [1358018.2500]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[ 855133.9375],\n",
            "        [1242808.5000],\n",
            "        [ 855133.9375],\n",
            "        [1242808.5000],\n",
            "        [2214607.5000],\n",
            "        [1216472.6250],\n",
            "        [2520616.7500],\n",
            "        [1212396.2500],\n",
            "        [2111122.7500]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "game 2000 completed\n",
            "[[-1.  1.  0.]\n",
            " [ 0.  1. -1.]\n",
            " [ 0.  1.  0.]]\n",
            "[(1, (1, 1)), (-1, (0, 0)), (1, (2, 1)), (-1, (1, 2)), (1, (0, 1))]\n",
            "5\n",
            "{1: 8, -1: -11}\n",
            "tensor([[3840327.7500],\n",
            "        [ 262110.6406],\n",
            "        [ 262110.6406],\n",
            "        [5403670.0000],\n",
            "        [2440341.0000],\n",
            "        [3812196.2500],\n",
            "        [3317433.5000],\n",
            "        [7768379.5000],\n",
            "        [1221914.7500]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[6848067.5000],\n",
            "        [1222339.7500],\n",
            "        [5625677.5000],\n",
            "        [6848067.5000],\n",
            "        [5625677.5000],\n",
            "        [5625677.5000],\n",
            "        [3755233.2500],\n",
            "        [3755233.2500],\n",
            "        [7477176.5000]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[7772114.5000],\n",
            "        [7772114.5000],\n",
            "        [6886305.5000],\n",
            "        [6849642.5000],\n",
            "        [2774884.5000],\n",
            "        [3377704.2500],\n",
            "        [1830694.0000],\n",
            "        [ 876999.0000],\n",
            "        [2369342.7500]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "tensor([[3852384.2500],\n",
            "        [3789603.2500],\n",
            "        [ 262254.6875],\n",
            "        [3462800.7500],\n",
            "        [ 262254.6875],\n",
            "        [1831456.5000],\n",
            "        [3852384.2500],\n",
            "        [2735918.2500],\n",
            "        [6887960.0000]], device='cuda:0', grad_fn=<GatherBackward>)\n",
            "game 3000 completed\n",
            "[[ 1.  1.  1.]\n",
            " [ 1.  1. -1.]\n",
            " [-1. -1. -1.]]\n",
            "[(1, (0, 0)), (-1, (1, 2)), (1, (0, 2)), (-1, (2, 0)), (1, (1, 0)), (-1, (2, 2)), (1, (1, 1)), (-1, (2, 1)), (1, (0, 1))]\n",
            "9\n",
            "{1: -3, -1: -31}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-268-89db60d2db10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0magent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_agent2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_win\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-267-5b21b5865878>\u001b[0m in \u001b[0;36mtrain_agent2\u001b[0;34m(agent1, agent2, gamma, num_games, dim, num_win)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcurr_player\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mcurr_player\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_player\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-266-6dbdca3c6b1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, state, action, reward, discount, next_state, i)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03CrVsd6bhmj"
      },
      "source": [
        "play_against_agent(agent2, 3,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xubcY47i_k_"
      },
      "source": [
        "agent2.qnet(torch.tensor([-1,1,0,0,1,0,0,0,0]).type(torch.float32).to(device)).view(3,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbvyDb_Fj6PZ",
        "outputId": "44bac1f4-33ba-44ca-de26-74a1ccdee296"
      },
      "source": [
        "gm = Gomoku(3,3)\n",
        "gm.board"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_vHEe61lHGy",
        "outputId": "7686b836-0ec2-4954-9e36-e24af6148f79"
      },
      "source": [
        "gm.won()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cL3KdkflNvt",
        "outputId": "8698f60c-7c33-4756-ad7c-136f674b4188"
      },
      "source": [
        "gm.make_move((1,1),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH0HAY-ulSh-",
        "outputId": "7ea17254-1feb-4e44-9e99-dc3632b00fc4"
      },
      "source": [
        "gm.board"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHKnpzSolVEK",
        "outputId": "86c519f8-b7ca-4471-b96e-d38fe9539fe8"
      },
      "source": [
        "gm.won()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5k_WuZSlgZB",
        "outputId": "946c56e2-2554-4f3e-9470-866b90325659"
      },
      "source": [
        "gm.make_move((1,0),-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAHokxn-l1nb",
        "outputId": "066277c2-8a53-42c1-d6c3-72361868bcd7"
      },
      "source": [
        "gm.won()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, -1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7G_PGodl2E-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}