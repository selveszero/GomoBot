{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial11basedDQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ddajjvySiA"
      },
      "source": [
        "# imports\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asUY88Z-yR3W"
      },
      "source": [
        "**Gomoku** Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39cDAMATEfRU"
      },
      "source": [
        "class Gomoku:\n",
        "  def __init__(self, dim, win):\n",
        "    self.dim = dim\n",
        "    self.win = dim\n",
        "    self.total_moves = 0\n",
        "    self.winner = 0\n",
        "    self.board = np.zeros((self.dim, self.dim))\n",
        "    self.log = [] \n",
        "\n",
        "  def get_available(self):\n",
        "    positions = list(map(list, np.where(self.board == 0)))\n",
        "    return list(zip(positions[0], positions[1]))\n",
        "\n",
        "  def make_move(self, position, player):\n",
        "    self.log.append((player, position))\n",
        "    if position not in self.get_available():\n",
        "      return -100\n",
        "      \n",
        "    self.board[position] = player\n",
        "    self.total_moves += 1 \n",
        "\n",
        "    if self.won() == self.win:\n",
        "      self.winner = player\n",
        "      return 100\n",
        "    \n",
        "    return -1\n",
        "\n",
        "  def won(self):\n",
        "    # Return self.winner if there's a winning subsequence for any player. \n",
        "    # check horizontal\n",
        "    ans = 0\n",
        "\n",
        "    for i in range(0, self.dim):\n",
        "    #  print([ abs(sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)])\n",
        "      ans = max(ans, max([ abs(sum(self.board[i, j: j +self.win])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    for j in range(0, self.dim):\n",
        "      ans  =max(ans, max([ abs(sum(self.board[i: i+self.win, j])) for i in range(0, self.dim - self.win + 1)]))\n",
        "    #check vertical\n",
        "\n",
        "    for i in range(0, self.dim - self.win+1):\n",
        "      ans = max(ans, max([ abs(sum(   self.board[ [i + k for k in range(0, self.win)], [j + l for l in range(0, self.win)]  ]    )) for j in range(0, self.dim - self.win + 1 )]))\n",
        "    #check diagonal\n",
        "    for i in range(self.win-1, self.dim):\n",
        "      ans = max(ans, max([ abs(sum(self.board[[i - k for k in range(0, self.win)], [j + l for l in range(0, self.win)] ])) for j in range(0, self.dim - self.win + 1)]))\n",
        "    #check anti-diagonal\n",
        "    return ans\n",
        "\n",
        " "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF7wtOmzswXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b90696-5681-4b5f-c740-023800a4eb1b"
      },
      "source": [
        "temp = Gomoku(3,3)\n",
        "temp.board\n",
        "\n",
        "temp.make_move((1,2), 1)\n",
        "\n",
        "temp.log"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, (1, 2))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDxMLwpveE83"
      },
      "source": [
        "#Network for Q learning\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, dim, num_win):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(dim*dim, 512)\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512, 512)\n",
        "    self.fc4 = nn.Linear(512, 512)\n",
        "    self.fc5 = nn.Linear(512, dim*dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x).to(device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = self.fc5(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLkyqFwd4jS"
      },
      "source": [
        "#Agent class as in week 11 tutorial.\n",
        "class Agent:\n",
        "  def __init__(self, policy, qnet, optimizer):\n",
        "    self.policy = policy\n",
        "    self.qnet = qnet\n",
        "    self.optimizer = optimizer  \n",
        "\n",
        "  def act(self, state):\n",
        "    with torch.no_grad():\n",
        "      return self.policy(self.qnet, state)\n",
        "  \n",
        "  def train(self, state, action, reward, discount, next_state):\n",
        "    q_pred = self.qnet(state).view(1,-1)\n",
        "    actionr = action.view(1,-1)\n",
        "    \n",
        "    q_pred = q_pred.gather(1, actionr)\n",
        "    #q_pred = self.qnet(state).gather(1, action)\n",
        "    #print(q_pred)\n",
        "    with torch.no_grad():\n",
        "      q_target = self.qnet(next_state).view(1,-1)\n",
        "      #print(torch.max(q_target), reward, discount)\n",
        "      #q_traget = q_target.max(dim=0)[0].view(-1,1)\n",
        "      #q_target = self.qnet(next_state).max(dim=1)[0].view(-1, 1)\n",
        "      q_target = torch.max(q_target).view(-1,1)\n",
        "      q_target = reward + discount * q_target\n",
        "      #print(q_target)\n",
        "   # print(q_pred, q_target)\n",
        "    loss = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYOvgkXteD_N"
      },
      "source": [
        "def train_agents(p1, p2, gamma, num_games, dim, num_win):\n",
        "  \n",
        "  for i in range(num_games):\n",
        "    game = Gomoku(dim, num_win)\n",
        "    curr_player = 1\n",
        "    end_game = 0 \n",
        "    rewards = {1: 0, -1:0}\n",
        "    agent = {1 : p1, -1 : p2}\n",
        "\n",
        "    while game.winner == 0 and game.total_moves < dim**2:\n",
        "      #let two agents play each other and learn.\n",
        "      state = torch.from_numpy(game.board).type(torch.float32).to(device)\n",
        "      #state = state.to(device)\n",
        "     # print(state)\n",
        "      action = agent[curr_player].act(state)\n",
        "      position = (action//dim, action%dim)\n",
        "      reward = game.make_move(position, curr_player)\n",
        "      end_game, next_state = abs(game.winner), torch.from_numpy(game.board).type(torch.float32)\n",
        "      discount = gamma*(1-end_game)\n",
        "      #print(action, position, game.board)\n",
        "      agent[curr_player].train(state, action, reward, discount, next_state)\n",
        "\n",
        "      rewards[curr_player] += reward\n",
        "      curr_player = curr_player * -1\n",
        "    if i % 100 == 0:\n",
        "      print(f\"game {i} completed\")\n",
        "      print(game.board, game.winner, game.total_moves)\n",
        "   # print(game.board, game.winner, game.total_moves)\n",
        "      \n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLOVcCmndydo"
      },
      "source": [
        "def epsilon_greedy(n_actions, epsilon):\n",
        "  def policy_fn(q_net, state):\n",
        "    if torch.rand(1) < epsilon:\n",
        "      return torch.randint(n_actions, size=(1,), device=device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        q_pred = q_net(state)\n",
        "        return torch.argmax(q_pred).view(1,)\n",
        "  return policy_fn"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOARqBngBPXS",
        "outputId": "080543f7-0de3-4b46-9c6f-a45623552ac2"
      },
      "source": [
        "num_games = 1000\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "dim = 3\n",
        "num_win = 3\n",
        "num_actions = dim**2\n",
        "\n",
        "p1 = DQN(dim, num_win).to(device)\n",
        "p2 = DQN(dim, num_win).to(device)\n",
        "\n",
        "\n",
        "policy1 = epsilon_greedy(num_actions, epsilon)\n",
        "policy2 = epsilon_greedy(num_actions, epsilon)\n",
        "\n",
        "optimizer1 = torch.optim.Adam(p1.parameters(), lr=1e-3)\n",
        "optimizer2 = torch.optim.Adam(p2.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "agent1 = Agent(policy1, p1, optimizer1)\n",
        "agent2 = Agent(policy2, p2, optimizer2)\n",
        "\n",
        "train_agents(agent1, agent2, gamma, num_games, dim, num_win)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game 0 completed\n",
            "[[ 1. -1. -1.]\n",
            " [-1.  1.  1.]\n",
            " [ 1. -1.  1.]] 1 9\n",
            "game 100 completed\n",
            "[[ 0.  1.  0.]\n",
            " [-1. -1. -1.]\n",
            " [ 0.  0.  1.]] -1 5\n",
            "game 200 completed\n",
            "[[-1. -1.  1.]\n",
            " [-1.  1. -1.]\n",
            " [ 1.  0.  0.]] 1 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV1EkFdNf4pZ",
        "outputId": "9e33b0db-f90c-408d-dca0-3fd02227be9e"
      },
      "source": [
        "5//3, 5%3"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI9CAn_Hjym0"
      },
      "source": [
        "012\n",
        "345\n",
        "678"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ZLa9erlBXS"
      },
      "source": [
        "hi = torch.tensor([ 0.0212, -0.0384,  0.0191,  0.0014,  0.0157, -0.0082, -0.0513, -0.0141,\n",
        "         0.0241])\n",
        "ii = torch.tensor([8])"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkVPvt9jlDLM",
        "outputId": "a66f78f4-8470-4d7d-b16f-12c943ed6ed4"
      },
      "source": [
        "hi"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0212, -0.0384,  0.0191,  0.0014,  0.0157, -0.0082, -0.0513, -0.0141,\n",
              "         0.0241])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwGaYJUVlL9W"
      },
      "source": [
        "hi = hi.view(1,-1)\n",
        "ii = ii.view(1,-1)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKDRYZLMlO7o",
        "outputId": "a9ad0953-74bd-444a-975e-8747f55fef5f"
      },
      "source": [
        "hi.gather(1, ii)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0241]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJJa3z5ilfEw",
        "outputId": "a01c1438-f2e8-43d2-fcd1-c95ec54f953a"
      },
      "source": [
        "hi, ii"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0212, -0.0384,  0.0191,  0.0014,  0.0157, -0.0082, -0.0513, -0.0141,\n",
              "           0.0241]]), tensor([[8]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkGrfgbilzKR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}